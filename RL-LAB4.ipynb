{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Universidad del Valle de Guatemala  \n",
        "Aprendizaje por Refuerzo\n",
        "Alberto Suriano  \n",
        "\n",
        "Laboratorio 4  \n",
        "Marlon Hernández - 15177  \n",
        "\n",
        "- Link del repositorio: https://github.com/ivanhez/RL-LAB4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1\n",
        "Responda a cada de las siguientes preguntas de forma clara y lo más completamente posible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **¿Cómo afecta la elección de la estrategia de exploración (exploring starts vs soft policy) a la precisión de la evaluación de políticas en los métodos de Monte Carlo?**\n",
        "    \n",
        "- a. Considere la posibilidad de comparar el desempeño de las políticas evaluadas con y sin explorar los inicios o con diferentes niveles de exploración en políticas blandas.\n",
        "\n",
        "    El Exploring Starts es una técnica que asegura que todas las posibles acciones desde cada estado se exploren al iniciar los episodios desde cada posible estado-acción. Esto permite que la política se evalúe de manera completa en el espacio de estados, esto lleva a una evaluación más precisa de la política. Sin embargo, esta estrategia puede ser poco práctica en entornos más complejos o cuando el espacio de estados es muy grande.\n",
        "    \n",
        "    En las Soft Policy todas las acciones tienen una probabilidad mayor que cero de ser seleccionadas y permiten una exploración continua sin necesidad de iniciar en cada estado-acción. Aunque esta estrategia facilita la implementación y es más flexible, puede llevar más tiempo converger a una evaluación precisa de la política debido a que algunas áreas del espacio de estados pueden llegar a ser no exploradas.\n",
        "\n",
        "2. **En el contexto del aprendizaje de Monte Carlo fuera de la póliza, ¿cómo afecta la razón de muestreo de importancia a la convergencia de la evaluación de políticas? Explore cómo la razón de muestreo de importancia afecta la estabilidad y la convergencia**  \n",
        "    La razón de muestreo de importancia es crítica en métodos de Monte Carlo off-policy, ya que ajusta las recompensas obtenidas bajo una política de comportamiento para estimar los valores bajo una política objetivo. Una mala elección en la política de comportamiento puede llevar a razones de muestreo de importancia amplias, lo que afecta la varianza y las estimaciones. Si la política de comportamiento es significativamente diferente de la política objetivo, las razones de muestreo de importancia pueden variar ampliamente, aumentando la varianza y afectando negativamente la convergencia.\n",
        "    \n",
        "3. **¿Cómo puede el uso de una soft policy influir en la eficacia del aprendizaje de políticas óptimas en comparación con las políticas deterministas en los métodos de Monte Carlo? Compare el desempeño y los resultados de aprendizaje de las políticas derivadas de estrategias épsilon-greedy con las derivadas de políticas deterministas**  \n",
        "    Las políticas derivadas de estrategias épsilon-greedy pueden ofrecer un mejor balance entre exploración y explotación, adaptándose mejor a cambios en el entorno y aprendiendo políticas óptimas de manera más efectiva en entornos complejos.\n",
        "    \n",
        "    Las soft policies permiten una exploración continua del espacio de estados, lo que puede ser beneficioso en entornos dinámicos o desconocidos. Esto puede conducir a un aprendizaje más robusto pero más lento, mientras que las políticas deterministas: Pueden converger más rápidamente en entornos estables donde la relación entre acciones y recompensas es bien entendida. Sin embargo, carecen de la capacidad de explorar alternativas que podrían resultar en descubrimientos de estrategias más eficientes.\n",
        "\n",
        "4. **¿Cuáles son los posibles beneficios y desventajas de utilizar métodos de Monte Carlo off-policy en comparación con los on-policy en términos de eficiencia de la muestra, costo computacional. y velocidad de aprendizaje?**\n",
        "    Las políticas Off-Policy permiten reutilizar experiencias pasadas acumuladas bajo diferentes políticas de comportamiento, lo que aumenta la eficiencia de las muestras. Sin embargo, requiere métodos adicionales para ajustar las diferencias entre las políticas de comportamiento y objetivo, lo que puede incrementar el costo computacional y afectar la velocidad de aprendizaje debido a problemas de varianza alta. Con las políticas On-Policy se aprende directamente basándose en las acciones tomadas, lo que puede resultar en una convergencia más rápida y estable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2\n",
        "En este ejercicio, simulará un sistema de gestión de inventarios para una pequeña tienda minorista. La tienda tiene como objetivo maximizar las ganancias manteniendo niveles óptimos de existencias de diferentes productos.\n",
        "Utilizará métodos de Monte Carlo para la evaluación de pólizas, exploring starts, soft policies y aprendizaje off-policy para estimar el valor de diferentes estrategias de gestión de inventarios. Su objetivo es implementar una solución en Python y responder preguntas específicas en función de los resultados.\n",
        "Instrucciones:\n",
        "1. Defina el entorno:\n",
        "\n",
        "- a. Utilice el ambiente dado más adelante para simular el entorno de la tienda. Considere que:\n",
        "  - i. El estado representa los niveles de existencias actuales de los productos.\n",
        "  - ii. Las acciones representan decisiones sobre cuánto reponer de cada producto.\n",
        "\n",
        "2. Generar episodios:\n",
        "\n",
        "- a. Cada episodio representa una serie de días en los que la tienda sigue una política de inventario específica.\n",
        "- b. Debe recopilar datos para varios episodios y registrar las recompensas (ganancias) de cada día.\n",
        "\n",
        "3. Exploring Starts:\n",
        "\n",
        "- a. Implemente explorar inicios para garantizar un conjunto diverso de estados y acciones iniciales.\n",
        "\n",
        "4. Soft Policies:\n",
        "\n",
        "- a. Utilice una soft policy (como epsilon-greedy) para garantizar un equilibrio entre la exploración y la explotación.\n",
        "\n",
        "5. Aprendizaje off-policy:\n",
        "        \n",
        "- a. Implemente el aprendizaje off-policy para evaluar una política objetivo utilizando datos generados por una política de comportamiento diferente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valor estimado de la política objetivo: -87.46537553519286\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# definicion del ambiente\n",
        "class InventoryEnvironment:\n",
        "    def __init__(self):\n",
        "        self.products = ['product_A', 'product_B']\n",
        "        self.max_stock = 10\n",
        "        self.demand = {'product_A': [0, 1, 2], 'product_B': [0, 1, 2]}\n",
        "        self.restock_cost = {'product_A': 5, 'product_B': 7}\n",
        "        self.sell_price = {'product_A': 10, 'product_B': 15}\n",
        "        self.state = None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = {product: random.randint(0, self.max_stock) for product in self.products}\n",
        "        return self.state\n",
        "\n",
        "    def step(self, action):\n",
        "        reward = 0\n",
        "        for product in self.products:\n",
        "            stock = self.state[product]\n",
        "            restock = action[product]\n",
        "            self.state[product] = min(self.max_stock, stock + restock)\n",
        "            demand = random.choice(self.demand[product])\n",
        "            sales = min(demand, self.state[product])\n",
        "            self.state[product] -= sales\n",
        "            reward += sales * self.sell_price[product] - restock * self.restock_cost[product]\n",
        "        return self.state, reward\n",
        "\n",
        "# init del ambiente\n",
        "env = InventoryEnvironment()\n",
        "\n",
        "# politica epsilon greedy\n",
        "def epsilon_greedy_policy(state, epsilon=0.1):\n",
        "    if random.uniform(0, 1) < epsilon:\n",
        "        return {product: random.randint(0, env.max_stock) for product in env.products}\n",
        "    else:\n",
        "        return {product: env.max_stock - state[product] for product in env.products}\n",
        "\n",
        "# generar un episodio\n",
        "def generate_episode(policy, exploring_starts=False):\n",
        "    state = env.reset()\n",
        "    if exploring_starts:\n",
        "        action = {product: random.randint(0, env.max_stock) for product in env.products}\n",
        "    else:\n",
        "        action = policy(state)\n",
        "    states, actions, rewards = [state], [action], []\n",
        "    for _ in range(10):\n",
        "        next_state, reward = env.step(action)\n",
        "        action = policy(next_state)\n",
        "        states.append(next_state)\n",
        "        actions.append(action)\n",
        "        rewards.append(reward)\n",
        "    return states, actions, rewards\n",
        "\n",
        "# evaluar la politica usando Monte Carlo off-policy\n",
        "def off_policy_evaluation(policy, behavior_policy, episodes=1000, gamma=0.9):\n",
        "    G = 0\n",
        "    C = 0\n",
        "    for _ in range(episodes):\n",
        "        states, actions, rewards = generate_episode(behavior_policy)\n",
        "        W = 1.0\n",
        "        for t in range(len(states)):\n",
        "            G = gamma * G + rewards[t]\n",
        "            if actions[t] != policy(states[t]):\n",
        "                break\n",
        "            W /= 1.0 / (1.0 / len(env.products))\n",
        "            C += W\n",
        "    return G / C if C != 0 else 0\n",
        "\n",
        "# hiperparametros\n",
        "epsilon = 0.1\n",
        "gamma = 0.9\n",
        "episodes = 1000\n",
        "\n",
        "# politicas\n",
        "target_policy = lambda state: {product: env.max_stock - state[product] for product in env.products}\n",
        "behavior_policy = lambda state: epsilon_greedy_policy(state, epsilon)\n",
        "\n",
        "# evaluar la politica objetivo usando datos generados por la politica de comportamiento\n",
        "value_estimate = off_policy_evaluation(target_policy, behavior_policy, episodes, gamma)\n",
        "print(f\"Valor estimado de la política objetivo: {value_estimate}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preguntas para responder:\n",
        "1. ¿Cuál es el valor estimado de mantener diferentes niveles de existencias para cada producto?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valor estimado por niveles de existencias: {(0, 0): -83.73780709644494, (0, 1): -74.07435426290122, (0, 2): -140.61648845660397, (0, 3): -80.62832635842103, (0, 4): -187.60160938560406, (0, 5): -50.877270861632454, (0, 6): -56.13373757227087, (0, 7): -110.35914871796778, (0, 8): -113.4442146912103, (0, 9): -279.74006024258966, (0, 10): -97.39195434832573, (1, 0): -49.20110472549334, (1, 1): -85.91347659102001, (1, 2): -201.78923434712362, (1, 3): -88.00687524641862, (1, 4): -65.28551222641815, (1, 5): -96.46419691615556, (1, 6): -121.47518560501533, (1, 7): -93.55821708434306, (1, 8): -79.32127152868073, (1, 9): -111.03819314350116, (1, 10): -58.545444579573974, (2, 0): -135.9018823139432, (2, 1): -161.3022981193048, (2, 2): -83.0896135535379, (2, 3): -65.29983096334546, (2, 4): -35.492793038562795, (2, 5): -109.3354346716919, (2, 6): -63.72399907566641, (2, 7): -62.643854011921405, (2, 8): -153.000946915906, (2, 9): -40.353501050278666, (2, 10): -115.37148862606645, (3, 0): -70.16543058982477, (3, 1): -62.55382981098258, (3, 2): -98.33783242379377, (3, 3): -34.830946930643385, (3, 4): -167.6968985851107, (3, 5): -277.15768968946225, (3, 6): -42.347974914813385, (3, 7): -81.6782585911088, (3, 8): -65.20543143786944, (3, 9): -62.82274607943965, (3, 10): -73.99405240162123, (4, 0): -67.4143296788134, (4, 1): -70.78726441614388, (4, 2): -47.4303111350774, (4, 3): -59.9483420029358, (4, 4): -60.36690167435875, (4, 5): -78.40701904716421, (4, 6): -82.35258311110681, (4, 7): -90.19084642865099, (4, 8): -76.60149582352372, (4, 9): -67.5084689376445, (4, 10): -72.33926514454981, (5, 0): -132.3038036532734, (5, 1): -68.75011707943456, (5, 2): -27.467767690939684, (5, 3): -63.81369147863106, (5, 4): -105.0689472618608, (5, 5): -190.34667767958703, (5, 6): -89.21111252509216, (5, 7): -96.84527033933828, (5, 8): -29.366835842973625, (5, 9): -154.50254131584356, (5, 10): -120.69336363727038, (6, 0): -224.9150376106446, (6, 1): -72.69211040030291, (6, 2): -59.9329372076133, (6, 3): -51.864164980442915, (6, 4): -50.98999216795344, (6, 5): -71.95970178320184, (6, 6): -89.48132925528061, (6, 7): -80.48648465171102, (6, 8): -97.39325531246644, (6, 9): -118.31011004843795, (6, 10): -62.05069443959909, (7, 0): -113.35407230091023, (7, 1): -136.46788426701303, (7, 2): -97.16485081050999, (7, 3): -52.731141348709436, (7, 4): -45.260381033713145, (7, 5): -54.939065173187515, (7, 6): -62.770852798979085, (7, 7): -107.48182813713349, (7, 8): -43.74804787848902, (7, 9): -103.90642144804143, (7, 10): -57.05029315319952, (8, 0): -91.50427585951357, (8, 1): -62.96041673114255, (8, 2): -78.82079434226095, (8, 3): -99.20664580521621, (8, 4): -61.28048813100685, (8, 5): -105.13724199672585, (8, 6): -100.5657067199114, (8, 7): -50.740678929543506, (8, 8): -172.5854418327372, (8, 9): -71.75899110119228, (8, 10): -87.7972347141911, (9, 0): -64.26619109641291, (9, 1): -96.0565423790196, (9, 2): -90.07087373465046, (9, 3): -73.86465901547116, (9, 4): -47.32842332000054, (9, 5): -74.5805034769255, (9, 6): -91.31076429919132, (9, 7): -50.92447744359925, (9, 8): -175.9039419277753, (9, 9): -60.59037792241067, (9, 10): -119.61612038567546, (10, 0): -104.09874361530662, (10, 1): -79.23603564527671, (10, 2): -87.75276165563336, (10, 3): -170.61023235216322, (10, 4): -75.02015101997475, (10, 5): -173.36970305296921, (10, 6): -170.194744233013, (10, 7): -221.53548114975402, (10, 8): -72.39470329366794, (10, 9): -69.52428961436138, (10, 10): -99.11757168402607}\n",
            "Valor estimado: -93.75126643285196\n"
          ]
        }
      ],
      "source": [
        "def value_estimation_per_stock_level():\n",
        "    value_estimates = {}\n",
        "    for stock_A in range(env.max_stock + 1):\n",
        "        for stock_B in range(env.max_stock + 1):\n",
        "            state = {'product_A': stock_A, 'product_B': stock_B}\n",
        "            value_estimates[(stock_A, stock_B)] = off_policy_evaluation(target_policy, behavior_policy, episodes, gamma)\n",
        "    return value_estimates\n",
        "\n",
        "stock_values = value_estimation_per_stock_level()\n",
        "print(f\"Valor estimado por niveles de existencias: {stock_values}\")\n",
        "\n",
        "res = 0\n",
        "for val in stock_values.values(): \n",
        "    res += val \n",
        "\n",
        "res = res / len(stock_values) \n",
        "  \n",
        "print(f\"Valor estimado promedio: {res}\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. ¿Cómo afecta el valor epsilon en la política blanda al rendimiento?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Rendimiento para diferentes valores de epsilon: {0.01: -54.435769377566544, 0.1: -57.744221662259335, 0.2: -81.2231140523781, 0.5: -120.52548268795776}\n"
          ]
        }
      ],
      "source": [
        "epsilon_values = [0.01, 0.1, 0.2, 0.5]\n",
        "performance = {}\n",
        "for eps in epsilon_values:\n",
        "    behavior_policy = lambda state: epsilon_greedy_policy(state, eps)\n",
        "    performance[eps] = off_policy_evaluation(target_policy, behavior_policy, episodes, gamma)\n",
        "print(f\"Rendimiento para diferentes valores de epsilon: {performance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. ¿Cuál es el impacto de utilizar el aprendizaje fuera de la política en comparación con el aprendizaje dentro de la política"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Valor on-policy: -102.0975479664497\n",
            "Valor off-policy: -56.80675675601024\n"
          ]
        }
      ],
      "source": [
        "on_policy_value = off_policy_evaluation(target_policy, target_policy, episodes, gamma)\n",
        "off_policy_value = off_policy_evaluation(target_policy, behavior_policy, episodes, gamma)\n",
        "print(f\"Valor on-policy: {on_policy_value}\")\n",
        "print(f\"Valor off-policy: {off_policy_value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Referencias:\n",
        "- https://medium.com/nerd-for-tech/monte-carlo-methods-for-reinforcement-learning-d30d874dd817\n",
        "- https://medium.com/@hsinhungw/intro-to-reinforcement-learning-monte-carlo-to-policy-gradient-1c7ede4eed6e\n",
        "- https://www.geeksforgeeks.org/monte-carlo-policy-evaluation/ \n",
        "- https://cwkx.github.io/data/teaching/dl-and-rl/rl-lecture5.pdf\n",
        "- https://bechirtr97.medium.com/on-policy-vs-off-policy-monte-carlo-control-methods-for-supply-chain-optimization-a-use-case-of-7b8d0d2e80e6"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
